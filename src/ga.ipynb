{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the neural network architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    "    )\n",
    "\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, 5, stride=1, padding=2)\n",
    "        self.batchNorm1 = nn.BatchNorm2d(64)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(64, 128, 5, stride=1, padding=2)\n",
    "        self.batchNorm2 = nn.BatchNorm2d(128)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(128, 128, 5, stride=1, padding=2)\n",
    "        self.batchNorm3 = nn.BatchNorm2d(128)\n",
    "\n",
    "        self.fc1 = nn.Linear(4 * 4 * 128, 2048)\n",
    "        self.batchNorm4 = nn.BatchNorm1d(2048)\n",
    "\n",
    "        self.fc2 = nn.Linear(2048, 1024)\n",
    "        self.batchNorm5 = nn.BatchNorm1d(1024)\n",
    "\n",
    "        self.out = nn.Linear(1024, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.batchNorm1(x) # batch normalization\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, kernel_size=2, stride=2)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.batchNorm2(x) # batch normalization\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, kernel_size=2, stride=2)\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = self.batchNorm3(x) # batch normalization\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, kernel_size=2, stride=2)\n",
    "\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "\n",
    "        x = self.fc1(x)\n",
    "        x = self.batchNorm4(x) # batch normalization\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = self.fc2(x)\n",
    "        x = self.batchNorm5(x) # batch normalization\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = F.dropout(x, p=0.5)\n",
    "\n",
    "        x = self.out(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "MINI_BATCH_SIZE = 128\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor()]\n",
    "    )\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(\n",
    "    root=\"./data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transform\n",
    "    )\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "    trainset,\n",
    "    batch_size=MINI_BATCH_SIZE,\n",
    "    shuffle=True, # reshuffle data at every epoch\n",
    "    num_workers=2\n",
    "    )\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(\n",
    "    root=\"./data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transform\n",
    "    )\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "    testset,\n",
    "    batch_size=MINI_BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=2\n",
    "    )\n",
    "\n",
    "classes = (\"plane\", \"car\", \"bird\", \"cat\", \"deer\",\n",
    "           \"dog\", \"frog\", \"horse\", \"ship\", \"truck\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function definitions to train, test, and freeze the parameters of the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "''' Train the neural network using backpropagation with cross entropy as the loss function '''\n",
    "def train_nn(net: nn.Module, epochs: int, optimizer: torch.optim.Optimizer):\n",
    "  print(f'Initialising training ...')\n",
    "  print(f'- Epochs: {epochs}')\n",
    "  print(f'- Mini batch size: {MINI_BATCH_SIZE}')\n",
    "  print(f'- Optimiser: {optimizer}')\n",
    "  print(f'- Loss function: {F.cross_entropy.__name__}')\n",
    "\n",
    "  evaluation_loss_track = []\n",
    "  running_loss_track = []\n",
    "  accuracy_track = []\n",
    "\n",
    "  # loop over the dataset multiple times\n",
    "  for epoch in range(epochs):\n",
    "    running_loss = 0\n",
    "\n",
    "    # loop over the dataset and get mini-batch\n",
    "    for mini_batch in trainloader:\n",
    "      images = mini_batch[0].to(device)\n",
    "      labels = mini_batch[1].to(device)\n",
    "\n",
    "      optimizer.zero_grad() # zero the parameter gradients\n",
    "\n",
    "      preds = net(images) # forward mini-batch\n",
    "\n",
    "      loss = F.cross_entropy(preds, labels) # calculate loss\n",
    "      loss.backward() # calculate gradients with respect to each weight\n",
    "      optimizer.step() # update weights\n",
    "\n",
    "      running_loss += loss.item()\n",
    "\n",
    "      # track\n",
    "      evaluation_loss_track.append(loss.item())\n",
    "\n",
    "    accuracy = test_nn(net=net, verbose=False)\n",
    "    print(f'\\nEpoch {epoch} finished -- Running loss {running_loss} -- Accuracy {accuracy}')\n",
    "\n",
    "    # track\n",
    "    running_loss_track.append(running_loss)\n",
    "    accuracy_track.append(accuracy)\n",
    "\n",
    "  # plot\n",
    "  fig, ax1 = plt.subplots()\n",
    "\n",
    "  ax1.set_xlabel('Iterations over entire dataset (Epoch)')\n",
    "  \n",
    "  ax1.set_ylabel('Accuracy', color='b')\n",
    "  ax1.plot(np.array(accuracy_track), '--b', label='Accuracy', linewidth=0.5)\n",
    "\n",
    "  ax2 = ax1.twinx()\n",
    "  ax2.set_ylabel('Running loss per epoch', color='r')\n",
    "  ax2.plot(np.array(running_loss_track), '--r', label='Loss per epoch', linewidth=0.5)\n",
    "\n",
    "  fig.tight_layout()\n",
    "  fig.legend()\n",
    "  plt.show()\n",
    "\n",
    "''' Test the neural network '''\n",
    "def test_nn(net: nn.Module, verbose: bool):\n",
    "    # test the neural network\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    # since we're not training, we don't need to calculate the gradients for our outputs\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images = data[0].to(device)\n",
    "            labels = data[1].to(device)\n",
    "\n",
    "            # calculate outputs by running images through the network\n",
    "            outputs = net(images)\n",
    "            # the class with the highest energy is what we choose as prediction\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct // total\n",
    "\n",
    "    if verbose:\n",
    "        print('Testing on 10,000 test images ...')\n",
    "        print(f'- Correct: {correct}')\n",
    "        print(f'- Total: {total}')\n",
    "        print(f'- Accuracy: {accuracy}')\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "''' Freeze all the parameters except the last layer and randomize last layer '''\n",
    "def freeze_parameters(net: nn.Module):\n",
    "    # freeze all the parameters in the NN\n",
    "    for param in net.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    # unfreeze all the parameters from the last layer and randomise the weights\n",
    "    for param in net.out.parameters():\n",
    "        param.requires_grad = True\n",
    "        param.data = torch.rand(param.size(), device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the pre-trained neural network model and test it to check the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing on 10,000 test images ...\n",
      "- Correct: 7718\n",
      "- Total: 10000\n",
      "- Accuracy: 77\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "77"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATH = './nn-models/cifar10-nn-model'\n",
    "\n",
    "# load the pretrained NN model\n",
    "net = Net()\n",
    "net.load_state_dict(torch.load(PATH))\n",
    "net.to(device=device)\n",
    "\n",
    "test_nn(net=net, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== Generation 1 ====\n",
      "==== Generation 2 ====\n",
      "==== Generation 3 ====\n",
      "==== Generation 4 ====\n",
      "==== Generation 5 ====\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 124\u001b[0m\n\u001b[1;32m    120\u001b[0m         gen_performance\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mmin\u001b[39m(fits))\n\u001b[1;32m    122\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pop, gen_performance\n\u001b[0;32m--> 124\u001b[0m pop, gen_performance \u001b[38;5;241m=\u001b[39m \u001b[43mga\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[5], line 94\u001b[0m, in \u001b[0;36mga\u001b[0;34m()\u001b[0m\n\u001b[1;32m     92\u001b[0m offspring \u001b[38;5;241m=\u001b[39m tools\u001b[38;5;241m.\u001b[39mselBest(pop, ELITISM) \u001b[38;5;241m+\u001b[39m toolbox\u001b[38;5;241m.\u001b[39mselect(pop, \u001b[38;5;28mlen\u001b[39m(pop)\u001b[38;5;241m-\u001b[39mELITISM, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     93\u001b[0m \u001b[38;5;66;03m# clone the selected individuals\u001b[39;00m\n\u001b[0;32m---> 94\u001b[0m offspring \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtoolbox\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclone\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffspring\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;66;03m# crossover make pairs of all (even, odd) in offspring\u001b[39;00m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ind1, ind2 \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(offspring[::\u001b[38;5;241m2\u001b[39m], offspring[\u001b[38;5;241m1\u001b[39m::\u001b[38;5;241m2\u001b[39m]):\n",
      "File \u001b[0;32m/usr/lib/python3.10/copy.py:172\u001b[0m, in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    170\u001b[0m                 y \u001b[39m=\u001b[39m x\n\u001b[1;32m    171\u001b[0m             \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 172\u001b[0m                 y \u001b[39m=\u001b[39m _reconstruct(x, memo, \u001b[39m*\u001b[39;49mrv)\n\u001b[1;32m    174\u001b[0m \u001b[39m# If is its own copy, don't memoize.\u001b[39;00m\n\u001b[1;32m    175\u001b[0m \u001b[39mif\u001b[39;00m y \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m x:\n",
      "File \u001b[0;32m/usr/lib/python3.10/copy.py:288\u001b[0m, in \u001b[0;36m_reconstruct\u001b[0;34m(x, memo, func, args, state, listiter, dictiter, deepcopy)\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[39mif\u001b[39;00m deep:\n\u001b[1;32m    287\u001b[0m     \u001b[39mfor\u001b[39;00m item \u001b[39min\u001b[39;00m listiter:\n\u001b[0;32m--> 288\u001b[0m         item \u001b[39m=\u001b[39m deepcopy(item, memo)\n\u001b[1;32m    289\u001b[0m         y\u001b[39m.\u001b[39mappend(item)\n\u001b[1;32m    290\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/lib/python3.10/copy.py:139\u001b[0m, in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    137\u001b[0m d \u001b[39m=\u001b[39m \u001b[39mid\u001b[39m(x)\n\u001b[1;32m    138\u001b[0m y \u001b[39m=\u001b[39m memo\u001b[39m.\u001b[39mget(d, _nil)\n\u001b[0;32m--> 139\u001b[0m \u001b[39mif\u001b[39;00m y \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m _nil:\n\u001b[1;32m    140\u001b[0m     \u001b[39mreturn\u001b[39;00m y\n\u001b[1;32m    142\u001b[0m \u001b[39mcls\u001b[39m \u001b[39m=\u001b[39m \u001b[39mtype\u001b[39m(x)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import array\n",
    "import random\n",
    "import json\n",
    "import numpy as np\n",
    "from deap import base\n",
    "from deap.benchmarks.tools import diversity, convergence, hypervolume\n",
    "from deap import creator\n",
    "from deap import tools\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# store all the training dataset in a single batch\n",
    "ALL_DATA = []\n",
    "for batch in trainloader:\n",
    "    ALL_DATA.extend(batch)\n",
    "\n",
    "# count the number of dimensions of the last layer\n",
    "N_DIMENSION = 0\n",
    "for param in net.out.parameters():\n",
    "    N_DIMENSION += param.numel()\n",
    "\n",
    "LOW_BOUND = -1.0\n",
    "HIGH_BOUND = 1.0\n",
    "N_BITS = 8\n",
    "N_GENERATIONS = 250\n",
    "MU = 100\n",
    "CX_PB = 0.9\n",
    "UNIFORM_CX_PB = 0.5\n",
    "MUTATE_PB = 0.1\n",
    "MUTATE_FLIP_PB = 1.0 / (N_DIMENSION * N_BITS)\n",
    "ELITISM = 5\n",
    "\n",
    "def decode(individual):\n",
    "    real_numbers = []\n",
    "    for i in range(N_DIMENSION):\n",
    "        chromosome = individual[i*N_BITS:(i+1)*N_BITS]\n",
    "        bit_string = ''.join(map(str, chromosome))\n",
    "        num_as_int = int(bit_string, 2) # convert to int from base 2 list\n",
    "        num_in_range = LOW_BOUND + (HIGH_BOUND - LOW_BOUND) * num_as_int / 2**N_BITS\n",
    "        real_numbers.append(num_in_range)\n",
    "\n",
    "    return real_numbers\n",
    "\n",
    "def calculate_fitness(individual):\n",
    "    # put the parameters into the neural network\n",
    "    parameters = decode(individual=individual)\n",
    "    parameters = torch.as_tensor(parameters, dtype=torch.float32, device=device)\n",
    "\n",
    "    net.out.weight = torch.nn.Parameter(data=parameters[0:20480].reshape(10, 2048))\n",
    "    net.out.bias = torch.nn.Parameter(data=parameters[20480:20490])\n",
    "\n",
    "    # go over the dataset once\n",
    "    with torch.no_grad():\n",
    "      images = ALL_DATA[0].to(device)\n",
    "      labels = ALL_DATA[1].to(device)\n",
    "\n",
    "      preds = net(images) # predict entire dataset\n",
    "      loss = F.cross_entropy(preds, labels) # calculate loss\n",
    "\n",
    "    return loss.item(),\n",
    "\n",
    "creator.create(\"FitnessMin\", base.Fitness, weights=(-1.0,))\n",
    "creator.create(\"Individual\", list, fitness=creator.FitnessMin)\n",
    "\n",
    "toolbox = base.Toolbox()\n",
    "\n",
    "toolbox.register(\"attr_bool\", random.randint, 0, 1)\n",
    "toolbox.register(\"individual\", tools.initRepeat, creator.Individual, toolbox.attr_bool, N_BITS*N_DIMENSION)\n",
    "toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
    "\n",
    "toolbox.register(\"evaluate\", calculate_fitness)\n",
    "toolbox.register(\"crossover\", tools.cxUniform, indpb=UNIFORM_CX_PB)\n",
    "toolbox.register(\"mutate\", tools.mutFlipBit, indpb=MUTATE_FLIP_PB)\n",
    "toolbox.register(\"select\", tools.selTournament, fit_attr='fitness')\n",
    "\n",
    "def ga():\n",
    "    # generate initial random population of individuals (parameters)\n",
    "    pop = toolbox.population(n=MU)\n",
    "\n",
    "    # evaluate the entire population\n",
    "    fitnesses = list(map(toolbox.evaluate, pop))\n",
    "    for ind, fit in zip(pop, fitnesses):\n",
    "        ind.fitness.values = fit\n",
    "\n",
    "    # track the performance of each generation\n",
    "    gen_performance = [] \n",
    "\n",
    "    # begin the generational process\n",
    "    for gen in range(1, N_GENERATIONS):\n",
    "        print(f'==== Generation {gen} ====')\n",
    "\n",
    "        # select the next generation individuals\n",
    "        offspring = tools.selBest(pop, ELITISM) + toolbox.select(pop, len(pop)-ELITISM, 2)\n",
    "        # clone the selected individuals\n",
    "        offspring = list(map(toolbox.clone, offspring))\n",
    "        \n",
    "        # crossover make pairs of all (even, odd) in offspring\n",
    "        for ind1, ind2 in zip(offspring[::2], offspring[1::2]):\n",
    "            if random.random() <= CX_PB:\n",
    "                toolbox.crossover(ind1, ind2)\n",
    "                del ind1.fitness.values\n",
    "                del ind2.fitness.values\n",
    "\n",
    "        # mutation\n",
    "        for mutant in offspring:\n",
    "            if random.random() <= MUTATE_PB:\n",
    "                toolbox.mutate(mutant)\n",
    "                del mutant.fitness.values\n",
    "\n",
    "        # evaluate the individuals with an invalid fitness\n",
    "        invalid_ind = [ind for ind in offspring if not ind.fitness.valid]\n",
    "        fitnesses = map(toolbox.evaluate, invalid_ind)\n",
    "        for ind, fit in zip(invalid_ind, fitnesses):\n",
    "            ind.fitness.values = fit\n",
    "        \n",
    "        # population is entirely replaced by the offspring\n",
    "        pop[:] = offspring\n",
    "\n",
    "        # track the best individual at this generation\n",
    "        fits = [ind.fitness.values[0] for ind in pop]\n",
    "        gen_performance.append(min(fits))\n",
    "\n",
    "    return pop, gen_performance\n",
    "        \n",
    "pop, gen_performance = ga()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_individual = tools.selBest(pop, 1)[0]\n",
    "\n",
    "plt.plot(np.array(gen_performance), 'r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put best parameters back into the neural network\n",
    "parameters = torch.as_tensor(best_individual, dtype=torch.float32, device=device)\n",
    "net.out.weight = torch.nn.Parameter(data=parameters[0:20480].reshape(10, 2048))\n",
    "net.out.bias = torch.nn.Parameter(data=parameters[20480:20490])\n",
    "\n",
    "# test the neural network\n",
    "test_nn(net=net, verbose=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.12 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

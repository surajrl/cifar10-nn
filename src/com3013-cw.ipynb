{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bYh_GVraWdJG"
      },
      "source": [
        "Use GPU if available"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "21hnF7ao0MEv",
        "outputId": "42e14a33-5ce1-4602-d43a-a13638872c87",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using cuda device\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "device = (\n",
        "    \"cuda\"\n",
        "    if torch.cuda.is_available()\n",
        "    else \"mps\"\n",
        "    if torch.backends.mps.is_available()\n",
        "    else \"cpu\"\n",
        "    )\n",
        "\n",
        "print(f\"Using {device} device\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ctVFVh_7-0b6"
      },
      "source": [
        "Load CIFAR10 dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ku2V2SEU-wwy",
        "outputId": "1624c48a-406f-47ae-d229-71a179a7aaf9",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor()]\n",
        "    )\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(\n",
        "    root=\"./data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=transform\n",
        "    )\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(\n",
        "    trainset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True, # reshuffle data at every epoch\n",
        "    num_workers=2\n",
        "    )\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(\n",
        "    root=\"./data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=transform\n",
        "    )\n",
        "\n",
        "testloader = torch.utils.data.DataLoader(\n",
        "    testset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        "    num_workers=2\n",
        "    )\n",
        "\n",
        "classes = (\"plane\", \"car\", \"bird\", \"cat\", \"deer\",\n",
        "           \"dog\", \"frog\", \"horse\", \"ship\", \"truck\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UAwN5w5v1aGK"
      },
      "source": [
        "Define the neural network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "GRLmbYxLzdgZ",
        "tags": []
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 64, 5, stride=1, padding=2)\n",
        "        self.batchNorm1 = nn.BatchNorm2d(64)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(64, 128, 5, stride=1, padding=2)\n",
        "        self.batchNorm2 = nn.BatchNorm2d(128)\n",
        "\n",
        "        self.conv3 = nn.Conv2d(128, 128, 5, stride=1, padding=2)\n",
        "        self.batchNorm3 = nn.BatchNorm2d(128)\n",
        "\n",
        "        self.fc1 = nn.Linear(4 * 4 * 128, 2048)\n",
        "        self.batchNorm4 = nn.BatchNorm1d(2048)\n",
        "\n",
        "        self.fc2 = nn.Linear(2048, 2048)\n",
        "        self.batchNorm5 = nn.BatchNorm1d(2048)\n",
        "\n",
        "        self.fc3 = nn.Linear(2048, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.batchNorm1(x) # batch normalization\n",
        "        x = F.relu(x)\n",
        "        x = F.max_pool2d(x, kernel_size=2, stride=2)\n",
        "\n",
        "        x = self.conv2(x)\n",
        "        x = self.batchNorm2(x) # batch normalization\n",
        "        x = F.relu(x)\n",
        "        x = F.max_pool2d(x, kernel_size=2, stride=2)\n",
        "\n",
        "        x = self.conv3(x)\n",
        "        x = self.batchNorm3(x) # batch normalization\n",
        "        x = F.relu(x)\n",
        "        x = F.max_pool2d(x, kernel_size=2, stride=2)\n",
        "        \n",
        "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
        "\n",
        "        x = self.fc1(x)\n",
        "        x = self.batchNorm4(x) # batch normalization\n",
        "        x = F.relu(x)\n",
        "\n",
        "        x = self.fc2(x)\n",
        "        x = self.batchNorm5(x) # batch normalization\n",
        "        x = F.relu(x)\n",
        "\n",
        "        x = F.dropout(x, p=0.5)\n",
        "        \n",
        "        x = self.fc3(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Function to test the NN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "def test_nn(net: nn.Module, verbose: bool):\n",
        "    # test the neural network\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    # since we're not training, we don't need to calculate the gradients for our outputs\n",
        "    with torch.no_grad():\n",
        "        for data in testloader:\n",
        "            images = data[0].to(device)\n",
        "            labels = data[1].to(device)\n",
        "\n",
        "            # calculate outputs by running images through the network\n",
        "            outputs = net(images)\n",
        "            # the class with the highest energy is what we choose as prediction\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "  \n",
        "    accuracy = 100 * correct // total\n",
        "  \n",
        "    if verbose:\n",
        "        print('Testing on 10,000 test images ...')\n",
        "        print(f'- Correct: {correct}')\n",
        "        print(f'- Total: {total}')\n",
        "        print(f'- Accuracy: {accuracy}')\n",
        "  \n",
        "    return accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def train_nn(net: nn.Module, epochs: int, optimizer: torch.optim.Optimizer, loss_function):\n",
        "  print(f'Initialising training ...')\n",
        "  print(f'- Epochs: {epochs}')\n",
        "  print(f'- Batch size: {batch_size}')\n",
        "  print(f'- Optimiser: {optimizer}')\n",
        "  print(f'- Loss function: {loss_function}')\n",
        "\n",
        "  running_loss_track = []\n",
        "  accuracy_track = []\n",
        "\n",
        "  # loop over the dataset multiple times\n",
        "  for epoch in range(epochs):\n",
        "    running_loss = 0\n",
        "\n",
        "    # loop over the dataset and get mini-batch\n",
        "    for mini_batch_num, data in enumerate(trainloader, 0):\n",
        "      images = data[0].to(device)\n",
        "      labels = data[1].to(device)\n",
        "\n",
        "      optimizer.zero_grad() # zero the parameter gradients\n",
        "\n",
        "      preds = net(images) # forward mini-batch\n",
        "      \n",
        "      loss = F.cross_entropy(preds, labels) # calculate loss\n",
        "      loss.backward() # calculate gradients with respect to each weight\n",
        "      optimizer.step() # update weights\n",
        "\n",
        "      running_loss += loss.item()\n",
        "\n",
        "    accuracy = test_nn(net=net, verbose=False)\n",
        "    print(f'\\nEpoch {epoch} finished -- Running loss {running_loss} -- Accuracy {accuracy}')\n",
        "\n",
        "    # track\n",
        "    running_loss_track.append(running_loss)\n",
        "    accuracy_track.append(accuracy)\n",
        "\n",
        "  # plot\n",
        "  fig, ax1 = plt.subplots()\n",
        "  \n",
        "  ax1.set_xlabel('Iterations over entire dataset')\n",
        "  ax1.set_ylabel('Accuracy', color='b')\n",
        "  ax1.plot(np.array(accuracy_track), '--b', label='Accuracy', linewidth=0.5)\n",
        "\n",
        "  ax2 = ax1.twinx()\n",
        "  ax2.set_ylabel('Running Loss', color='r')\n",
        "  ax2.plot(np.array(running_loss_track), '--r', label='Loss', linewidth=0.5)\n",
        "\n",
        "  fig.tight_layout()\n",
        "  fig.legend()\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t1nbp-N4qtIg"
      },
      "source": [
        "Create new NN model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "KC8q-C6-qrlU",
        "outputId": "3b54953b-3d26-4fda-e975-7d673d4f7de5",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# import matplotlib.pyplot as plt\n",
        "# import numpy as np\n",
        "# import time\n",
        "\n",
        "# # create a new NN\n",
        "# torch.manual_seed(1)\n",
        "# net = Net()\n",
        "# net.to(device)\n",
        "\n",
        "# optimizer = torch.optim.SGD(net.parameters(), lr=0.01, momentum=0.0)\n",
        "# loss_function = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# train_nn(net=net, epochs=50, optimizer=optimizer, loss_function=loss_function)\n",
        "# test_nn(net=net, verbose=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Save the new NN model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "# PATH = \"../nn-models/cifar10-nn-model\"\n",
        "# torch.save(net.state_dict(), PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Freeze all the parameters except those from the last layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "def freeze_parameters(net: nn.Module):\n",
        "    # freeze all the parameters in the NN\n",
        "    for param in net.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "    # unfreeze all the parameters from the last layer and randomise the weights\n",
        "    last_layer = list(net.children())[-1]\n",
        "    for param in last_layer.parameters():\n",
        "        param.requires_grad = True\n",
        "        param.data = torch.rand(param.size(), device=device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8dccyb6ZRn7z"
      },
      "source": [
        "Gradient descent method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "tCCYz3w3RsgT",
        "outputId": "3376056a-40e6-4e74-81e8-330deab05941"
      },
      "outputs": [],
      "source": [
        "# PATH = \"../nn-models/cifar10-nn-model\"\n",
        "\n",
        "# # load the pretrained NN model\n",
        "# net = Net()\n",
        "# net.load_state_dict(torch.load(PATH))\n",
        "# net.to(device=device)\n",
        "\n",
        "# freeze_parameters(net=net)\n",
        "\n",
        "# optimizer = torch.optim.Adagrad(net.parameters())\n",
        "# loss_function = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# train_nn(net=net, epochs=50, optimizer=optimizer, loss_function=loss_function)\n",
        "# test_nn(net=net, verbose=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Population-based heuristic algorithm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[23], line 89\u001b[0m\n\u001b[1;32m     87\u001b[0m     new_individual \u001b[38;5;241m=\u001b[39m creator\u001b[38;5;241m.\u001b[39mIndividual(new_genes)\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;66;03m# calculate their fitness\u001b[39;00m\n\u001b[0;32m---> 89\u001b[0m     new_individual\u001b[38;5;241m.\u001b[39mfitness\u001b[38;5;241m.\u001b[39mvalues \u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_fitness\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_individual\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     91\u001b[0m     population\u001b[38;5;241m.\u001b[39mappend(new_individual)\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPopulation of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(population)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m individuals generated\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
            "Cell \u001b[0;32mIn[23], line 71\u001b[0m, in \u001b[0;36mcalculate_fitness\u001b[0;34m(individual)\u001b[0m\n\u001b[1;32m     68\u001b[0m images \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     69\u001b[0m labels \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 71\u001b[0m preds \u001b[38;5;241m=\u001b[39m \u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# forward mini-batch\u001b[39;00m\n\u001b[1;32m     72\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_function(preds, labels) \u001b[38;5;66;03m# calculate loss\u001b[39;00m\n\u001b[1;32m     74\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
            "Cell \u001b[0;32mIn[16], line 36\u001b[0m, in \u001b[0;36mNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     33\u001b[0m x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmax_pool2d(x, kernel_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, stride\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     35\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv3(x)\n\u001b[0;32m---> 36\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatchNorm3\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# batch normalization\u001b[39;00m\n\u001b[1;32m     37\u001b[0m x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(x)\n\u001b[1;32m     38\u001b[0m x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmax_pool2d(x, kernel_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, stride\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py:171\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    164\u001b[0m     bn_training \u001b[39m=\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrunning_mean \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m) \u001b[39mand\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrunning_var \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m    166\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    167\u001b[0m \u001b[39mBuffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\u001b[39;00m\n\u001b[1;32m    168\u001b[0m \u001b[39mpassed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\u001b[39;00m\n\u001b[1;32m    169\u001b[0m \u001b[39mused for normalization (i.e. in eval mode when buffers are not None).\u001b[39;00m\n\u001b[1;32m    170\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 171\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mbatch_norm(\n\u001b[1;32m    172\u001b[0m     \u001b[39minput\u001b[39;49m,\n\u001b[1;32m    173\u001b[0m     \u001b[39m# If buffers are not to be tracked, ensure that they won't be updated\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrunning_mean\n\u001b[1;32m    175\u001b[0m     \u001b[39mif\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining \u001b[39mor\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrack_running_stats\n\u001b[1;32m    176\u001b[0m     \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    177\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrunning_var \u001b[39mif\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining \u001b[39mor\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrack_running_stats \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    178\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight,\n\u001b[1;32m    179\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias,\n\u001b[1;32m    180\u001b[0m     bn_training,\n\u001b[1;32m    181\u001b[0m     exponential_average_factor,\n\u001b[1;32m    182\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49meps,\n\u001b[1;32m    183\u001b[0m )\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/functional.py:2478\u001b[0m, in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   2475\u001b[0m \u001b[39mif\u001b[39;00m training:\n\u001b[1;32m   2476\u001b[0m     _verify_batch_size(\u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize())\n\u001b[0;32m-> 2478\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mbatch_norm(\n\u001b[1;32m   2479\u001b[0m     \u001b[39minput\u001b[39;49m, weight, bias, running_mean, running_var, training, momentum, eps, torch\u001b[39m.\u001b[39;49mbackends\u001b[39m.\u001b[39;49mcudnn\u001b[39m.\u001b[39;49menabled\n\u001b[1;32m   2480\u001b[0m )\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import random\n",
        "from deap import creator, base, tools, algorithms\n",
        "\n",
        "PATH = \"../nn-models/cifar10-nn-model\"\n",
        "\n",
        "# load the pretrained NN model\n",
        "net = Net()\n",
        "net.load_state_dict(torch.load(PATH))\n",
        "net.to(device=device)\n",
        "\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "\n",
        "freeze_parameters(net=net)\n",
        "\n",
        "# genetic algorithm\n",
        "n_bits = 8\n",
        "\n",
        "dimensions = 0\n",
        "last_layer = list(net.children())[-1]\n",
        "for param in last_layer.parameters():\n",
        "    dimensions += param.numel()\n",
        "\n",
        "population_size = 50\n",
        "generations = 50\n",
        "elitism = 5\n",
        "crossover_probability = 0.9\n",
        "uniform_crossover_probability = 0.5\n",
        "mutation_probability = 0.1\n",
        "mutation_flip_probability = 1.0 / (dimensions * n_bits)\n",
        "\n",
        "creator.create(\"FitnessMin\", base.Fitness, weights=(-1.0,))\n",
        "creator.create(\"Individual\", list, fitness=creator.FitnessMin)\n",
        "\n",
        "toolbox = base.Toolbox()\n",
        "# register the uniform crossover\n",
        "toolbox.register(\"crossover\", tools.cxUniform, indpb=uniform_crossover_probability)\n",
        "# register the bit mutation\n",
        "toolbox.register(\"mutation\", tools.mutFlipBit, indpb=mutation_flip_probability)\n",
        "# register the selection\n",
        "toolbox.register(\"select\", tools.selTournament, fit_attr='fitness')\n",
        "\n",
        "def decode(individual):\n",
        "    real_numbers = []\n",
        "    for i in range(dimensions):\n",
        "        chromosome = individual[i*n_bits:(i+1)*n_bits]\n",
        "        bit_string = ''.join(map(str, chromosome))\n",
        "        num_as_int = int(bit_string, 2) # convert to int from base 2 list\n",
        "        num_in_range = -10 + (10 + 10) * num_as_int / 2**n_bits\n",
        "        real_numbers.append(num_in_range)\n",
        "\n",
        "    return real_numbers\n",
        "\n",
        "def calculate_fitness(individual):\n",
        "    parameters = decode(individual=individual)\n",
        "    parameters = torch.as_tensor(parameters, dtype=torch.float32, device=device)\n",
        "    \n",
        "    # 10 is the output\n",
        "    # 2048 is the input\n",
        "\n",
        "    # put the parameters in the neural network\n",
        "    net.fc3.weight = torch.nn.Parameter(data=parameters[0:20480].reshape(10, 2048))\n",
        "    net.fc3.bias = torch.nn.Parameter(data=parameters[20480:20490])\n",
        "\n",
        "    # go over the dataset once\n",
        "    running_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for mini_batch_num, data in enumerate(trainloader, 0):\n",
        "            images = data[0].to(device)\n",
        "            labels = data[1].to(device)\n",
        "\n",
        "            preds = net(images) # forward mini-batch\n",
        "            loss = loss_function(preds, labels) # calculate loss\n",
        "\n",
        "            running_loss += loss.item()\n",
        "    \n",
        "    return running_loss,\n",
        "\n",
        "# generate an initial population\n",
        "population = [] \n",
        "for _ in range(population_size):\n",
        "    # generate random genes\n",
        "    new_genes = []\n",
        "    for _ in range(n_bits * dimensions):\n",
        "        new_genes.append(random.randint(0, 1))\n",
        "    \n",
        "    # generate an individual from those random genes\n",
        "    new_individual = creator.Individual(new_genes)\n",
        "    # calculate their fitness\n",
        "    new_individual.fitness.values = calculate_fitness(new_individual)\n",
        "\n",
        "    population.append(new_individual)\n",
        "\n",
        "print(f'Population of {len(population)} individuals generated')\n",
        "\n",
        "best_individuals = []\n",
        "\n",
        "# start generations loop\n",
        "for g in range(generations):\n",
        "    # select the next generation of individuals\n",
        "    offspring = tools.selBest(population, elitism) + toolbox.select(population, len(population) - elitism, 2)\n",
        "    offspring = list(map(toolbox.clone, offspring))\n",
        "\n",
        "    # crossover\n",
        "    for child1, child2 in zip(offspring[elitism::2], offspring[elitism+1::2]):\n",
        "        # with a certain probability generate an offspring by uniform crossover\n",
        "        if random.random() < crossover_probability:\n",
        "            toolbox.crossover(child1, child2)\n",
        "            # invalidate the fitness value of the offspring\n",
        "            del child1.fitness.values\n",
        "            del child2.fitness.values\n",
        "\n",
        "    # mutation\n",
        "    for mutant in offspring[elitism:]:\n",
        "        # with a certain probability generate an offspring by bit mutation\n",
        "        if random.random() < mutation_probability:\n",
        "            toolbox.mutation(mutant)\n",
        "            # invalidate the fitness value of the offspring\n",
        "            del mutant.fitness.values\n",
        "\n",
        "    # calculate the fitness of the offspring\n",
        "    for individual in offspring:\n",
        "        if not individual.fitness.valid:\n",
        "            individual.fitness.values = calculate_fitness(individual)\n",
        "\n",
        "    # replacement\n",
        "    population[:] = offspring\n",
        "\n",
        "    # track the best individuals\n",
        "    curr_best_individual = tools.selBest(population, 1)[0]\n",
        "    best_individuals.append(curr_best_individual.fitness.values[0])\n",
        "    print(f'Generation {g} -- Current best individual has a fitness value of {curr_best_individual.fitness.values}')\n",
        "\n",
        "# choose the best individual from the final population as the parameters (weights and biases) of the NN\n",
        "plt.plot(np.array(best_individuals), 'r')\n",
        "best_ind = tools.selBest(population, 1)[0]\n",
        "print(f'End of evolution -- Best individual has a fitness value of {best_ind.fitness.values}')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

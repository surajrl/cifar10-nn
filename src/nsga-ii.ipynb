{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the neural network architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    "    )\n",
    "\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, 5, stride=1, padding=2)\n",
    "        self.batchNorm1 = nn.BatchNorm2d(64)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(64, 128, 5, stride=1, padding=2)\n",
    "        self.batchNorm2 = nn.BatchNorm2d(128)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(128, 128, 5, stride=1, padding=2)\n",
    "        self.batchNorm3 = nn.BatchNorm2d(128)\n",
    "\n",
    "        self.fc1 = nn.Linear(4 * 4 * 128, 2048)\n",
    "        self.batchNorm4 = nn.BatchNorm1d(2048)\n",
    "\n",
    "        self.fc2 = nn.Linear(2048, 2048)\n",
    "        self.batchNorm5 = nn.BatchNorm1d(2048)\n",
    "\n",
    "        self.out = nn.Linear(2048, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.batchNorm1(x) # batch normalization\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, kernel_size=2, stride=2)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.batchNorm2(x) # batch normalization\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, kernel_size=2, stride=2)\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = self.batchNorm3(x) # batch normalization\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, kernel_size=2, stride=2)\n",
    "\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "\n",
    "        x = self.fc1(x)\n",
    "        x = self.batchNorm4(x) # batch normalization\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = self.fc2(x)\n",
    "        x = self.batchNorm5(x) # batch normalization\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = F.dropout(x, p=0.5)\n",
    "\n",
    "        x = self.out(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor()]\n",
    "    )\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(\n",
    "    root=\"./data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transform\n",
    "    )\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "    trainset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True, # reshuffle data at every epoch\n",
    "    num_workers=2\n",
    "    )\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(\n",
    "    root=\"./data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transform\n",
    "    )\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "    testset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=2\n",
    "    )\n",
    "\n",
    "classes = (\"plane\", \"car\", \"bird\", \"cat\", \"deer\",\n",
    "           \"dog\", \"frog\", \"horse\", \"ship\", \"truck\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function definitions to train, test, and freeze the parameters of the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "''' Train the neural network using backpropagation with cross entropy as the loss function '''\n",
    "def train_nn(net: nn.Module, epochs: int, optimizer: torch.optim.Optimizer):\n",
    "  print(f'Initialising training ...')\n",
    "  print(f'- Epochs: {epochs}')\n",
    "  print(f'- Batch size: {batch_size}')\n",
    "  print(f'- Optimiser: {optimizer}')\n",
    "  print(f'- Loss function: {F.cross_entropy}')\n",
    "\n",
    "  running_loss_track = []\n",
    "  accuracy_track = []\n",
    "\n",
    "  # loop over the dataset multiple times\n",
    "  for epoch in range(epochs):\n",
    "    running_loss = 0\n",
    "\n",
    "    # loop over the dataset and get mini-batch\n",
    "    for mini_batch_num, data in enumerate(trainloader, 0):\n",
    "      images = data[0].to(device)\n",
    "      labels = data[1].to(device)\n",
    "\n",
    "      optimizer.zero_grad() # zero the parameter gradients\n",
    "\n",
    "      preds = net(images) # forward mini-batch\n",
    "\n",
    "      loss = F.cross_entropy(preds, labels) # calculate loss\n",
    "      loss.backward() # calculate gradients with respect to each weight\n",
    "      optimizer.step() # update weights\n",
    "\n",
    "      running_loss += loss.item()\n",
    "\n",
    "    accuracy = test_nn(net=net, verbose=False)\n",
    "    print(f'\\nEpoch {epoch} finished -- Running loss {running_loss} -- Accuracy {accuracy}')\n",
    "\n",
    "    # track\n",
    "    running_loss_track.append(running_loss)\n",
    "    accuracy_track.append(accuracy)\n",
    "\n",
    "  # plot\n",
    "  fig, ax1 = plt.subplots()\n",
    "\n",
    "  ax1.set_xlabel('Iterations over entire dataset')\n",
    "  ax1.set_ylabel('Accuracy', color='b')\n",
    "  ax1.plot(np.array(accuracy_track), '--b', label='Accuracy', linewidth=0.5)\n",
    "\n",
    "  ax2 = ax1.twinx()\n",
    "  ax2.set_ylabel('Running Loss', color='r')\n",
    "  ax2.plot(np.array(running_loss_track), '--r', label='Loss', linewidth=0.5)\n",
    "\n",
    "  fig.tight_layout()\n",
    "  fig.legend()\n",
    "  plt.show()\n",
    "\n",
    "''' Test the neural network '''\n",
    "def test_nn(net: nn.Module, verbose: bool):\n",
    "    # test the neural network\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    # since we're not training, we don't need to calculate the gradients for our outputs\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images = data[0].to(device)\n",
    "            labels = data[1].to(device)\n",
    "\n",
    "            # calculate outputs by running images through the network\n",
    "            outputs = net(images)\n",
    "            # the class with the highest energy is what we choose as prediction\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct // total\n",
    "\n",
    "    if verbose:\n",
    "        print('Testing on 10,000 test images ...')\n",
    "        print(f'- Correct: {correct}')\n",
    "        print(f'- Total: {total}')\n",
    "        print(f'- Accuracy: {accuracy}')\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "''' Freeze all the parameters except the last layer and randomize last layer '''\n",
    "def freeze_parameters(net: nn.Module):\n",
    "    # freeze all the parameters in the NN\n",
    "    for param in net.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    # unfreeze all the parameters from the last layer and randomise the weights\n",
    "    for param in net.out.parameters():\n",
    "        param.requires_grad = True\n",
    "        param.data = torch.rand(param.size(), device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the pre-trained neural network model and test it to check the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = './nn-models/cifar10-nn-model'\n",
    "\n",
    "# load the pretrained NN model\n",
    "net = Net()\n",
    "net.load_state_dict(torch.load(PATH))\n",
    "net.to(device=device)\n",
    "\n",
    "test_nn(net=net, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NSGA-II"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import array\n",
    "import random\n",
    "import json\n",
    "import numpy as np\n",
    "from deap import base\n",
    "from deap.benchmarks.tools import diversity, convergence, hypervolume\n",
    "from deap import creator\n",
    "from deap import tools\n",
    "import matplotlib.pyplot as plt\n",
    "import multiprocessing\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "\n",
    "multiprocessing.set_start_method('spawn', True)\n",
    "\n",
    "creator.create(\"FitnessMin\", base.Fitness, weights=(-1.0, -1.0))\n",
    "creator.create(\"Individual\", list, fitness=creator.FitnessMin)\n",
    "\n",
    "toolbox = base.Toolbox()\n",
    "\n",
    "LOW_BOUND = -1.0\n",
    "HIGH_BOUND = 1.0\n",
    "N_BITS = 8\n",
    "N_GENERATIONS = 100\n",
    "MU = 52\n",
    "CXPB = 0.9\n",
    "MUTATE_PROB = 0.1\n",
    "\n",
    "# count the number of dimensions of the last layer\n",
    "N_DIMENSION = 0\n",
    "for param in net.out.parameters():\n",
    "    N_DIMENSION += param.numel()\n",
    "\n",
    "# loss function\n",
    "def f1(individual):\n",
    "    # take the parameters from the individual and replace the last layer of the NN with them\n",
    "    # parameters = decode(individual=individual)\n",
    "    parameters = torch.as_tensor(individual, dtype=torch.float32, device=device)\n",
    "\n",
    "    net.out.weight = torch.nn.Parameter(data=parameters[0:20480].reshape(10, 2048))\n",
    "    net.out.bias = torch.nn.Parameter(data=parameters[20480:20490])\n",
    "   \n",
    "    # loop over the dataset in mini-batches anc count the running loss\n",
    "    running_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for mini_batch_num, data in enumerate(trainloader, 0):\n",
    "            images = data[0].to(device)\n",
    "            labels = data[1].to(device)\n",
    "\n",
    "            preds = net(images) # forward mini-batch\n",
    "            loss = F.cross_entropy(preds, labels) # calculate loss\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "    print(f'individual {individual[0]} evaluated')\n",
    "    return running_loss\n",
    "\n",
    "# Gaussian regulariser (sum of the square of the weights)\n",
    "def f2():\n",
    "    squared_weights = []\n",
    "    for param_name, param in net.named_parameters():\n",
    "        squared_weight = torch.square(param.data)\n",
    "        squared_weights.append(squared_weight)\n",
    "\n",
    "    sum = 0\n",
    "    for param in squared_weights:\n",
    "        sum += torch.sum(param)\n",
    "        \n",
    "    return sum.detach().cpu().numpy()\n",
    "\n",
    "def obj(individual):\n",
    "    return (f1(individual=individual), f2()) \n",
    "\n",
    "# def decode(individual):\n",
    "#     real_numbers = []\n",
    "#     for i in range(N_DIMENSION):\n",
    "#         chromosome = individual[i*N_BITS:(i+1)*N_BITS]\n",
    "#         bit_string = ''.join(map(str, chromosome))\n",
    "#         num_as_int = int(bit_string, 2) # convert to int from base 2 list\n",
    "#         num_in_range = LOW_BOUND + (HIGH_BOUND - LOW_BOUND) * num_as_int / 2**N_BITS\n",
    "#         real_numbers.append(num_in_range)\n",
    "    \n",
    "#     return real_numbers\n",
    "\n",
    "def uniform(low, up, size=None):\n",
    "    try:\n",
    "        return [random.uniform(a, b) for a, b in zip(low, up)]\n",
    "    except TypeError:\n",
    "        return [random.uniform(a, b) for a, b in zip([low] * size, [up] * size)]\n",
    "\n",
    "toolbox.register(\"attr_float\", uniform, LOW_BOUND, HIGH_BOUND, N_DIMENSION)\n",
    "toolbox.register(\"individual\", tools.initIterate, creator.Individual, toolbox.attr_float)\n",
    "toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
    "\n",
    "toolbox.register(\"evaluate\", obj)\n",
    "toolbox.register(\"mate\", tools.cxSimulatedBinaryBounded, low=LOW_BOUND, up=HIGH_BOUND, eta=20.0)\n",
    "toolbox.register(\"mutate\", tools.mutPolynomialBounded, low=LOW_BOUND, up=HIGH_BOUND, eta=20.0, indpb=1.0/N_DIMENSION)\n",
    "toolbox.register(\"select\", tools.selNSGA2)\n",
    "\n",
    "def nsga_ii():\n",
    "    # generate initial random population of individuals (parameters)\n",
    "    pop = toolbox.population(n=MU)\n",
    "\n",
    "    # evaluate the individuals with an invalid fitness\n",
    "    invalid_ind = [ind for ind in pop if not ind.fitness.valid]\n",
    "\n",
    "    with ProcessPoolExecutor() as executor:\n",
    "        fitnesses = executor.map(toolbox.evaluate, invalid_ind)\n",
    "        for ind, fit in zip(invalid_ind, fitnesses):\n",
    "            ind.fitness.values = fit\n",
    "\n",
    "    # this is just to assign the crowding distance to\n",
    "    # the individuals no actual selection is done\n",
    "    pop = toolbox.select(pop, len(pop))\n",
    "    \n",
    "    # begin the generational process\n",
    "    for gen in range(1, N_GENERATIONS):\n",
    "        # vary the population\n",
    "        offspring = tools.selTournamentDCD(pop, len(pop))\n",
    "        \n",
    "        # selTournamentDCD means Tournament selection based on dominance (D) \n",
    "        # followed by crowding distance (CD). This selection requires the \n",
    "        # individuals to have a crowding_dist attribute\n",
    "        offspring = [toolbox.clone(ind) for ind in offspring]\n",
    "        \n",
    "        # crossover make pairs of all (even, odd) in offspring\n",
    "        for ind1, ind2 in zip(offspring[::2], offspring[1::2]):\n",
    "            if random.random() <= CXPB:\n",
    "                toolbox.mate(ind1, ind2)\n",
    "                del ind1.fitness.values\n",
    "                del ind2.fitness.values\n",
    "\n",
    "        # mutation\n",
    "        for mutant in offspring:\n",
    "            if random.random() <= MUTATE_PROB:\n",
    "                toolbox.mutate(mutant)\n",
    "                del mutant.fitness.values\n",
    "\n",
    "        # evaluate the individuals with an invalid fitness\n",
    "        invalid_ind = [ind for ind in offspring if not ind.fitness.valid]\n",
    "        with ProcessPoolExecutor() as executor:\n",
    "            fitnesses = executor.map(toolbox.evaluate, invalid_ind)\n",
    "            for ind, fit in zip(invalid_ind, fitnesses):\n",
    "                ind.fitness.values = fit\n",
    "\n",
    "        # Select the next generation population\n",
    "        pop = toolbox.select(pop + offspring, MU)\n",
    "\n",
    "        print(f'generation {gen} finished')\n",
    "\n",
    "    return pop\n",
    "        \n",
    "pop = nsga_ii()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop.sort(key=lambda x: x.fitness.values)\n",
    "\n",
    "front = np.array([ind.fitness.values for ind in pop])\n",
    "plt.scatter(front[:,0], front[:,1], c=\"b\")\n",
    "plt.axis(\"tight\")\n",
    "plt.xlabel('Loss function')\n",
    "plt.ylabel('Sum of the squared weights')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put best parameters back into the neural network\n",
    "parameters = torch.as_tensor(pop[0], dtype=torch.float32, device=device)\n",
    "net.out.weight = torch.nn.Parameter(data=parameters[0:20480].reshape(10, 2048))\n",
    "net.out.bias = torch.nn.Parameter(data=parameters[20480:20490])\n",
    "\n",
    "# test the neural network\n",
    "test_nn(net=net, verbose=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.12 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
